{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225d57f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08de2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4e9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3470bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner', 'ran', 'runs', 'easily', 'fairly', 'fairness'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87afa993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run----->run\n",
      "runner----->runner\n",
      "ran----->ran\n",
      "runs----->run\n",
      "easily----->easili\n",
      "fairly----->fairli\n",
      "fairness----->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print (word + '----->' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c5a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1508b61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language = 'english');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a06def7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run----->run\n",
      "runner----->runner\n",
      "ran----->ran\n",
      "runs----->run\n",
      "easily----->easili\n",
      "fairly----->fair\n",
      "fairness----->fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print (word + '----->' + s_stemmer.stem(word));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c461d668",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['generous', 'generation', 'generously', 'generate'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7a7401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous----->gener\n",
      "generation----->gener\n",
      "generously----->gener\n",
      "generate----->gener\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print (word + '----->' + p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "594ef5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous----->generous\n",
      "generation----->generat\n",
      "generously----->generous\n",
      "generate----->generat\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print (word + '----->' + s_stemmer.stem(word));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b02d51",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb929fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.4.1-cp39-cp39-win_amd64.whl (11.8 MB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.2-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (62.1.0)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.10-py2.py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Collecting thinc<8.2.0,>=8.1.0\n",
      "  Downloading thinc-8.1.0-cp39-cp39-win_amd64.whl (1.3 MB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.4-cp39-cp39-win_amd64.whl (450 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.1.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy) (4.63.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4\n",
      "  Downloading pydantic-1.9.2-cp39-cp39-win_amd64.whl (2.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.2-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy) (3.0.8)\n",
      "Collecting smart-open<6.0.0,>=5.2.1\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.8-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 1.9.0\n",
      "    Uninstalling smart-open-1.9.0:\n",
      "      Successfully uninstalled smart-open-1.9.0\n",
      "Successfully installed blis-0.7.8 catalogue-2.0.8 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.8 pathy-0.6.2 preshed-3.0.7 pydantic-1.9.2 smart-open-5.2.1 spacy-3.4.1 spacy-legacy-3.0.10 spacy-loggers-1.0.3 srsly-2.4.4 thinc-8.1.0 typer-0.4.2 wasabi-0.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65275b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.26.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.63.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.20.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (62.1.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.10.0.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\shahbaz\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shahbaz\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.0\n",
      "[!] As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "640d29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8705b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load ('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff71ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a46ee62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print (token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd8c47",
   "metadata": {},
   "source": [
    "## Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d392e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc53bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "816ed787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hereafter', '‘d', 'no', 'within', 'wherein', '’re', 'show', '’s', 'eight', 'becoming', 'a', 'keep', 'once', 'thence', 'whether', \"'d\", 'who', 'none', 'every', 'become', 'n‘t', 'during', 'up', 'beside', 'that', 'could', 'enough', 'while', 'those', 'after', 'amongst', 'whose', 'hence', 'meanwhile', 'others', 'please', 'of', 'may', 'such', 'perhaps', 'so', 'nevertheless', 'otherwise', 'on', 'whereby', 'cannot', 'anywhere', 'afterwards', 'for', 'five', 'if', 'take', 'its', 'whatever', 'off', 'two', 'various', 'hundred', 'latterly', 'doing', 'eleven', 'not', 'the', 'nor', 'ever', 'three', 'last', 'front', '‘re', \"'ve\", 'amount', 'see', 'with', 'own', 'move', 'full', 'besides', 'we', 'an', 'everything', 'had', 'seems', 'becomes', 'twelve', 'most', 'too', 'herself', 'many', 'as', 'less', 'over', 'however', 'whither', 'it', 'ours', 'down', 'used', 'hers', 'onto', 'behind', '’ll', 'they', 'thru', 'whom', 'nowhere', 'make', 'noone', 'several', 'via', 'about', 'whenever', 'along', 'you', 'him', 'go', 'because', 'above', 'or', 'thereby', '‘m', 'toward', 'their', 'than', 'else', 'anything', 'been', '‘s', 'sixty', '‘ll', '’d', 'quite', 'yours', 'someone', 'his', 'unless', 'must', \"'s\", 'somehow', 'these', 'why', 'everywhere', 'whereas', 'least', 'something', 'six', 'anyway', 'out', 'yourself', 'except', 'fifteen', 'already', 'anyhow', 'me', 'can', 'side', 'mine', 'also', 'get', 'before', \"'ll\", 'together', 'would', 'although', 'us', 'has', 'ca', 'everyone', 'empty', 'what', 'nobody', 'by', 'towards', 'through', 'say', 'across', 'among', 'elsewhere', 'but', 'am', 'just', 'really', 'yourselves', 'might', 'itself', 'do', 'seem', 'sometimes', 're', 'back', '‘ve', 'another', 'forty', 'when', \"'m\", 'beyond', 'beforehand', 'against', \"'re\", 'did', 'made', 'well', 'whereafter', 'therein', 'almost', 'fifty', 'under', 'either', '’m', 'them', 'next', 'serious', 'to', 'does', 'other', 'neither', 'thereupon', 'namely', 'using', 'there', 'due', 'will', 'are', 'without', 'alone', 'seeming', 'until', 'never', 'throughout', 'somewhere', 'still', 'mostly', 'since', 'always', 'top', 'how', 'formerly', 'thus', 'nine', 'ten', 'former', 'whence', 'whole', 'give', 'call', 'some', 'though', 'she', 'where', 'then', 'here', 'from', 'put', 'yet', 'much', 'four', 'were', 'third', 'in', 'indeed', 'sometime', 'should', 'was', 'all', 'often', 'he', 'now', 'became', 'between', 'anyone', \"n't\", 'therefore', 'each', 'twenty', 'bottom', 'your', 'part', 'few', 'done', 'being', 'himself', 'whereupon', 'at', 'even', 'into', 'again', 'my', 'more', 'below', 'thereafter', 'only', 'herein', 'latter', 'hereby', 'wherever', 'both', 'nothing', 'very', 'themselves', 'myself', 'seemed', 'around', 'our', 'is', 'her', 'one', 'n’t', 'regarding', 'rather', 'per', 'whoever', '’ve', 'ourselves', 'further', 'have', 'name', 'upon', 'first', 'this', 'same', 'moreover', 'any', 'and', 'i', 'which', 'be', 'hereupon'}\n"
     ]
    }
   ],
   "source": [
    "# print all the stopwords in spacy defaults\n",
    "print (nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5196ea8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2856b449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788faf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add 'btw' to stop words!\n",
    "nlp.Defaults.stop_words.add('btw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae26fc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'btw' to be true as a stop word!\n",
    "nlp.vocab['btw'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27420ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOTE: notice that 'btw' was added and therefore, the no. of stop words has changed\n",
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea146368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab['btw'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f2b2635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove a stop word from the list\n",
    "nlp.Defaults.stop_words.remove('beyond')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b5331e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set 'beyond'('s) is_stop as false\n",
    "nlp.vocab['beyond'].is_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b0ddaa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5dec63",
   "metadata": {},
   "source": [
    "## Vocabulary and Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db24367",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consult the vocab-and-matching for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a401ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e349b664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair with current vocab object\n",
    "matcher = Matcher(nlp.vocab);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a9121e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern to match on:\n",
    "# format: \"list of dictionaries\"\n",
    "pattern1 = [{'LOWER': 'solarpower'}];\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]; # lowercase solar? is there punctuation?\n",
    "pattern3 = [{'LOWER': 'solar'}, {'LOWER':'power'}];\n",
    "# the follwing will be detected:\n",
    "# SolarPower\n",
    "# Solar-power\n",
    "# Solar power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd611ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for further reference: consult the vocab and matching notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29cd27f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', [pattern1, pattern2, pattern3], on_match = None ); # callbacks have been set to: None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b4fa69fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp (u\"The Solar Power industry continues to grow as solarpower increases. Solar-power is amazing.\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0bce326",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ce6322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 8, 9), (8656102463236116519, 11, 14)]\n"
     ]
    }
   ],
   "source": [
    "print (found_matches)\n",
    "# format: (<match_id>, start, stop)\n",
    "# NOTE: Start and Stop are token positions and NOT INDICES OF ALPHABET!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdbd2954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 8 9 solarpower\n",
      "8656102463236116519 SolarPower 11 14 Solar-power\n"
     ]
    }
   ],
   "source": [
    "# to print out words as well:\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]; # get string representation\n",
    "    span = doc[start:end];                   # get matched span\n",
    "    print(match_id, string_id, start, end, span.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "25defccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove pattern(s)\n",
    "matcher.remove('SolarPower');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "492e3f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solarpower SolarPower\n",
    "pattern1 = [{'LOWER': 'solarpower'}];\n",
    "# solar.power\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP': '*'}, {'LOWER': 'power'}];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de7c6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('SolarPower', [pattern1, pattern2], on_match = None);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2e1e2d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u\"Solar--power is solarpower yay!\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "666195d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher(doc2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9178fcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "print (found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f6424f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 0 3 Solar--power\n",
      "8656102463236116519 SolarPower 4 5 solarpower\n"
     ]
    }
   ],
   "source": [
    "# to print out words as well:\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]; # get string representation\n",
    "    span = doc2[start:end];                   # get matched span\n",
    "    print(match_id, string_id, start, end, span.text);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe8d818",
   "metadata": {},
   "source": [
    "## Phrase matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93e853dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b4a7347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher (nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4543ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloaded wiki article:\n",
    "with open('../TextFiles/reaganomics.txt') as f:\n",
    "    doc3 = nlp(f.read());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "859999ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics'];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2339f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_patterns = [nlp(text) for text in phrase_list];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "154882aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[voodoo economics,\n",
       " supply-side economics,\n",
       " trickle-down economics,\n",
       " free-market economics]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrase_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c7b788e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(phrase_patterns[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "469e863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher.add('EconMatcher', None, *phrase_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d9d565d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_matches = matcher (doc3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01125a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3680293220734633682, 41, 45),\n",
       " (3680293220734633682, 49, 53),\n",
       " (3680293220734633682, 54, 56),\n",
       " (3680293220734633682, 61, 65),\n",
       " (3680293220734633682, 673, 677),\n",
       " (3680293220734633682, 2987, 2991)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "found_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e9ebfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 supply-side economics\n",
      "3680293220734633682 EconMatcher 49 53 trickle-down economics\n",
      "3680293220734633682 EconMatcher 54 56 voodoo economics\n",
      "3680293220734633682 EconMatcher 61 65 free-market economics\n",
      "3680293220734633682 EconMatcher 673 677 supply-side economics\n",
      "3680293220734633682 EconMatcher 2987 2991 trickle-down economics\n"
     ]
    }
   ],
   "source": [
    "# to print out words as well:\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]; # get string representation\n",
    "    span = doc3[start:end];                   # get matched span\n",
    "    print(match_id, string_id, start, end, span.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6267e1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680293220734633682 EconMatcher 41 45 policies are commonly associated with supply-side economics, referred to as trickle\n",
      "3680293220734633682 EconMatcher 49 53 economics, referred to as trickle-down economics or voodoo economics by political\n",
      "3680293220734633682 EconMatcher 54 56 trickle-down economics or voodoo economics by political opponents, and\n",
      "3680293220734633682 EconMatcher 61 65 by political opponents, and free-market economics by political advocates.\n",
      "\n",
      "\n",
      "3680293220734633682 EconMatcher 673 677 attracted a following from the supply-side economics movement, which formed in\n",
      "3680293220734633682 EconMatcher 2987 2991 became widely known as \"trickle-down economics\", due to the\n"
     ]
    }
   ],
   "source": [
    "# to print out words as well:\n",
    "# the -5 to start and +5 to end give us the words surrounding the key words\n",
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]; # get string representation\n",
    "    span = doc3[start-5:end+5];                   # get matched span\n",
    "    print(match_id, string_id, start, end, span.text);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c18e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
